Index: src/webrtc/voice_engine/voe_base_impl.cc
===================================================================
--- src/webrtc/voice_engine/voe_base_impl.cc	(revision 8098)
+++ src/webrtc/voice_engine/voe_base_impl.cc	(working copy)
@@ -148,7 +148,9 @@
         uint8_t nChannels,
         uint32_t samplesPerSec,
         void* audioSamples,
-        uint32_t& nSamplesOut)
+        uint32_t& nSamplesOut,
+        bool play,
+        int voeChannel)
 {
     WEBRTC_TRACE(kTraceStream, kTraceVoice, VoEId(_shared->instance_id(), -1),
                  "VoEBaseImpl::NeedMorePlayData(nSamples=%u, "
@@ -159,15 +161,17 @@
 
     // TODO(andrew): if the device is running in mono, we should tell the mixer
     // here so that it will only request mono from AudioCodingModule.
-    // Perform mixing of all active participants (channel-based mixing)
-    _shared->output_mixer()->MixActiveChannels();
+    if (play) {
+        // Perform mixing of all active participants (channel-based mixing)
+        _shared->output_mixer()->MixActiveChannels();
 
-    // Additional operations on the combined signal
-    _shared->output_mixer()->DoOperationsOnCombinedSignal();
+        // Additional operations on the combined signal
+        _shared->output_mixer()->DoOperationsOnCombinedSignal();
+    }
 
     // Retrieve the final output mix (resampled to match the ADM)
     _shared->output_mixer()->GetMixedAudio(samplesPerSec, nChannels,
-        &_audioFrame);
+        &_audioFrame, voeChannel);
 
     assert(static_cast<int>(nSamples) == _audioFrame.samples_per_channel_);
     assert(samplesPerSec ==
Index: src/webrtc/voice_engine/output_mixer.cc
===================================================================
--- src/webrtc/voice_engine/output_mixer.cc	(revision 8098)
+++ src/webrtc/voice_engine/output_mixer.cc	(working copy)
@@ -33,6 +33,22 @@
 
     _audioFrame.CopyFrom(generalAudioFrame);
     _audioFrame.id_ = id;
+
+    if (size > 0)
+    {
+        assert(uniqueAudioFrames != NULL
+          && size <= AudioConferenceMixer::kMaximumAmountOfMixedParticipants);
+
+        _numUniqueFrames = size;
+        for (uint32_t j = 0; j < size; ++j)
+        {
+            _uniqueFrames[j].CopyFrom(*(uniqueAudioFrames[j]));
+        }
+    }
+    else
+    {
+        _numUniqueFrames = 0;
+    }
 }
 
 void OutputMixer::MixedParticipants(
@@ -117,6 +133,7 @@
     _callbackCritSect(*CriticalSectionWrapper::CreateCriticalSection()),
     _fileCritSect(*CriticalSectionWrapper::CreateCriticalSection()),
     _mixerModule(*AudioConferenceMixer::Create(instanceId)),
+    _numUniqueFrames(0),
     _audioLevel(),
     _dtmfGenerator(instanceId),
     _instanceId(instanceId),
@@ -511,13 +528,22 @@
 
 int OutputMixer::GetMixedAudio(int sample_rate_hz,
                                int num_channels,
-                               AudioFrame* frame) {
+                               AudioFrame* frame,
+                               int voeChannel) {
   WEBRTC_TRACE(kTraceStream, kTraceVoice, VoEId(_instanceId,-1),
                "OutputMixer::GetMixedAudio(sample_rate_hz=%d, num_channels=%d)",
                sample_rate_hz, num_channels);
 
-  // --- Record playout if enabled
-  {
+  AudioFrame* inputFrame = &_audioFrame;
+  if (voeChannel != -1) {
+    for (uint32_t i = 0; i < _numUniqueFrames; ++i) {
+      if (_uniqueFrames[i].id_ == voeChannel) {
+        inputFrame = &_uniqueFrames[i];
+        break;
+      }
+    }
+  } else {
+    // --- Record playout if enabled
     CriticalSectionScoped cs(&_fileCritSect);
     if (_outputFileRecording && _outputFileRecorderPtr)
       _outputFileRecorderPtr->RecordAudioToFile(_audioFrame);
@@ -527,7 +553,7 @@
   frame->sample_rate_hz_ = sample_rate_hz;
   // TODO(andrew): Ideally the downmixing would occur much earlier, in
   // AudioCodingModule.
-  RemixAndResample(_audioFrame, &resampler_, frame);
+  RemixAndResample(*inputFrame, &resampler_, frame);
   return 0;
 }
 
@@ -542,34 +568,45 @@
         _mixingFrequencyHz = _audioFrame.sample_rate_hz_;
     }
 
-    // --- Insert inband Dtmf tone
-    if (_dtmfGenerator.IsAddingTone())
+    for (uint32_t i = 0; i <= _numUniqueFrames; ++i)
     {
-        InsertInbandDtmfTone();
-    }
+        AudioFrame* frame = &_audioFrame;
 
-    // Scale left and/or right channel(s) if balance is active
-    if (_panLeft != 1.0 || _panRight != 1.0)
-    {
-        if (_audioFrame.num_channels_ == 1)
+        if (i < _numUniqueFrames)
         {
-            AudioFrameOperations::MonoToStereo(&_audioFrame);
+            frame = &_uniqueFrames[i];
         }
-        else
+
+        // --- Insert inband Dtmf tone
+        if (_dtmfGenerator.IsAddingTone())
         {
-            // Pure stereo mode (we are receiving a stereo signal).
+            InsertInbandDtmfTone(*frame);
         }
 
-        assert(_audioFrame.num_channels_ == 2);
-        AudioFrameOperations::Scale(_panLeft, _panRight, _audioFrame);
+        // Scale left and/or right channel(s) if balance is active
+        if (_panLeft != 1.0 || _panRight != 1.0)
+        {
+            if (frame->num_channels_ == 1)
+            {
+                AudioFrameOperations::MonoToStereo(frame);
+            }
+            else
+            {
+                // Pure stereo mode (we are receiving a stereo signal).
+            }
+
+            assert(frame->num_channels_ == 2);
+            AudioFrameOperations::Scale(_panLeft, _panRight, *frame);
+        }
+
+        // --- Far-end Voice Quality Enhancement (AudioProcessing Module)
+        // TODO(ajm): Check with VoEBase if |need_audio_processing| is false.
+        // If so, we don't need to call this method and can avoid the
+        // subsequent resampling.
+        // See: https://code.google.com/p/webrtc/issues/detail?id=3147
+        APMAnalyzeReverseStream(*frame);
     }
 
-    // --- Far-end Voice Quality Enhancement (AudioProcessing Module)
-    // TODO(ajm): Check with VoEBase if |need_audio_processing| is false.
-    // If so, we don't need to call this method and can avoid the subsequent
-    // resampling. See: https://code.google.com/p/webrtc/issues/detail?id=3147
-    APMAnalyzeReverseStream();
-
     // --- External media processing
     {
         CriticalSectionScoped cs(&_callbackCritSect);
@@ -599,13 +636,13 @@
 //                             Private methods
 // ----------------------------------------------------------------------------
 
-void OutputMixer::APMAnalyzeReverseStream() {
+void OutputMixer::APMAnalyzeReverseStream(AudioFrame& audioFrame) {
   // Convert from mixing to AudioProcessing sample rate, determined by the send
   // side. Downmix to mono.
   AudioFrame frame;
   frame.num_channels_ = 1;
   frame.sample_rate_hz_ = _audioProcessingModulePtr->sample_rate_hz();
-  RemixAndResample(_audioFrame, &audioproc_resampler_, &frame);
+  RemixAndResample(audioFrame, &audioproc_resampler_, &frame);
 
   if (_audioProcessingModulePtr->AnalyzeReverseStream(&frame) == -1) {
     WEBRTC_TRACE(kTraceWarning, kTraceVoice, VoEId(_instanceId,-1),
@@ -614,15 +651,15 @@
 }
 
 int
-OutputMixer::InsertInbandDtmfTone()
+OutputMixer::InsertInbandDtmfTone(AudioFrame& audioFrame)
 {
     uint16_t sampleRate(0);
     _dtmfGenerator.GetSampleRate(sampleRate);
-    if (sampleRate != _audioFrame.sample_rate_hz_)
+    if (sampleRate != audioFrame.sample_rate_hz_)
     {
         // Update sample rate of Dtmf tone since the mixing frequency changed.
         _dtmfGenerator.SetSampleRate(
-            (uint16_t)(_audioFrame.sample_rate_hz_));
+            (uint16_t)(audioFrame.sample_rate_hz_));
         // Reset the tone to be added taking the new sample rate into account.
         _dtmfGenerator.ResetTone();
     }
@@ -638,21 +675,21 @@
     }
 
     // replace mixed audio with Dtmf tone
-    if (_audioFrame.num_channels_ == 1)
+    if (audioFrame.num_channels_ == 1)
     {
         // mono
-        memcpy(_audioFrame.data_, toneBuffer, sizeof(int16_t)
+        memcpy(audioFrame.data_, toneBuffer, sizeof(int16_t)
             * toneSamples);
     } else
     {
         // stereo
-        for (int i = 0; i < _audioFrame.samples_per_channel_; i++)
+        for (int i = 0; i < audioFrame.samples_per_channel_; i++)
         {
-            _audioFrame.data_[2 * i] = toneBuffer[i];
-            _audioFrame.data_[2 * i + 1] = 0;
+            audioFrame.data_[2 * i] = toneBuffer[i];
+            audioFrame.data_[2 * i + 1] = 0;
         }
     }
-    assert(_audioFrame.samples_per_channel_ == toneSamples);
+    assert(audioFrame.samples_per_channel_ == toneSamples);
 
     return 0;
 }
Index: src/webrtc/voice_engine/output_mixer.h
===================================================================
--- src/webrtc/voice_engine/output_mixer.h	(revision 8098)
+++ src/webrtc/voice_engine/output_mixer.h	(working copy)
@@ -69,7 +69,7 @@
                                          bool mixable);
 
     int GetMixedAudio(int sample_rate_hz, int num_channels,
-                      AudioFrame* audioFrame);
+                      AudioFrame* audioFrame, int voeChannel = -1);
 
     // VoEVolumeControl
     int GetSpeechOutputLevel(uint32_t& level);
@@ -120,8 +120,8 @@
 
 private:
     OutputMixer(uint32_t instanceId);
-    void APMAnalyzeReverseStream();
-    int InsertInbandDtmfTone();
+    void APMAnalyzeReverseStream(AudioFrame&);
+    int InsertInbandDtmfTone(AudioFrame&);
 
     // uses
     Statistics* _engineStatisticsPtr;
@@ -133,6 +133,9 @@
     CriticalSectionWrapper& _fileCritSect;
     AudioConferenceMixer& _mixerModule;
     AudioFrame _audioFrame;
+    AudioFrame _uniqueFrames[
+        AudioConferenceMixer::kMaximumAmountOfMixedParticipants];
+    uint32_t _numUniqueFrames;
     PushResampler resampler_;  // converts mixed audio to fit ADM format
     PushResampler audioproc_resampler_;  // converts mixed audio to fit APM rate
     AudioLevel _audioLevel;    // measures audio level for the combined signal
Index: src/webrtc/voice_engine/voe_base_impl.h
===================================================================
--- src/webrtc/voice_engine/voe_base_impl.h	(revision 8098)
+++ src/webrtc/voice_engine/voe_base_impl.h	(working copy)
@@ -89,7 +89,9 @@
                                      uint8_t nChannels,
                                      uint32_t samplesPerSec,
                                      void* audioSamples,
-                                     uint32_t& nSamplesOut);
+                                     uint32_t& nSamplesOut,
+                                     bool play = true,
+                                     int voeChannel = -1);
 
     virtual int OnDataAvailable(const int voe_channels[],
                                 int number_of_voe_channels,
Index: src/webrtc/modules/audio_device/include/audio_device_defines.h
===================================================================
--- src/webrtc/modules/audio_device/include/audio_device_defines.h	(revision 8098)
+++ src/webrtc/modules/audio_device/include/audio_device_defines.h	(working copy)
@@ -70,7 +70,9 @@
                                      const uint8_t nChannels,
                                      const uint32_t samplesPerSec,
                                      void* audioSamples,
-                                     uint32_t& nSamplesOut) = 0;
+                                     uint32_t& nSamplesOut,
+                                     bool play = true,
+                                     int voeChannel = -1) = 0;
 
     // Method to pass captured data directly and unmixed to network channels.
     // |channel_ids| contains a list of VoE channels which are the
Index: src/webrtc/modules/audio_conference_mixer/source/audio_conference_mixer_impl.cc
===================================================================
--- src/webrtc/modules/audio_conference_mixer/source/audio_conference_mixer_impl.cc	(revision 8098)
+++ src/webrtc/modules/audio_conference_mixer/source/audio_conference_mixer_impl.cc	(working copy)
@@ -338,12 +338,58 @@
     {
         CriticalSectionScoped cs(_cbCrit.get());
         if(_mixReceiver != NULL) {
-            const AudioFrame** dummy = NULL;
+            AudioFrame* uniqueFrames[kMaximumAmountOfMixedParticipants];
+
+            for (size_t i = 0; i < mixList.size(); ++i) {
+                AudioFrameList workList = mixList;
+                AudioFrameList::iterator it = workList.begin();
+                advance(it, i);
+                int id = (*it)->id_;
+                workList.erase(it);
+
+                // Get an AudioFrame for mixing from the memory pool.
+                AudioFrame* uniqueFrame = NULL;
+                if(_audioFramePool->PopMemory(uniqueFrame) == -1) {
+                    WEBRTC_TRACE(kTraceMemory, kTraceAudioMixerServer, _id,
+                                 "failed PopMemory() call");
+                    assert(false);
+                    return -1;
+                }
+
+                int num_mixed_channels = std::max(MaxNumChannels(&workList),
+                    std::max(MaxNumChannels(&additionalFramesList),
+                             MaxNumChannels(&rampOutList)));
+
+                uniqueFrame->UpdateFrame(id, _timeStamp, NULL, 0,
+                                _outputFrequency, AudioFrame::kNormalSpeech,
+                                AudioFrame::kVadPassive, num_mixed_channels);
+
+                MixFromList(*uniqueFrame, &workList);
+                MixAnonomouslyFromList(*uniqueFrame, &additionalFramesList);
+                MixAnonomouslyFromList(*uniqueFrame, &rampOutList);
+
+                if(uniqueFrame->samples_per_channel_ == 0) {
+                    // Nothing was mixed, set the audio samples to silence.
+                    uniqueFrame->samples_per_channel_ = _sampleSize;
+                    uniqueFrame->Mute();
+                } else {
+                    // Only call the limiter if we have something to mix.
+                    if(!LimitMixedAudio(*uniqueFrame))
+                        retval = -1;
+                }
+
+                uniqueFrames[i] = uniqueFrame;
+            }
+
             _mixReceiver->NewMixedAudio(
                 _id,
                 *mixedAudio,
-                dummy,
-                0);
+                const_cast<const AudioFrame**>(uniqueFrames),
+                mixList.size());
+
+            for (size_t j = 0; j < mixList.size(); ++j) {
+                _audioFramePool->PushMemory(uniqueFrames[j]);
+            }
         }
 
         if((_mixerStatusCallback != NULL) &&
